# Docker LLaMA2 Chat / ç¾Šé©¼äºŒä»£

<p style="text-align: center;">
  <a href="README.md"  target="_blank">ä¸­æ–‡æ–‡æ¡£</a> | <a href="README_EN.md">ENGLISH</a>
</p>

[![](https://img.shields.io/badge/LLaMA2-Official_7B_/_13B-blue)](https://huggingface.co/meta-llama) [![](https://img.shields.io/badge/LLaMA2-Chinese_7B-blue)](https://huggingface.co/soulteary/Chinese-Llama-2-7b-4bit) [![](https://img.shields.io/badge/LLaMA2-Chinese_GGMLQ4-blue)](https://huggingface.co/soulteary/Chinese-Llama-2-7b-ggml-q4) [![](https://img.shields.io/badge/License-Apache_v2-blue)](https://github.com/soulteary/docker-llama2-chat/blob/main/LICENSE)

<img src=".github/llama2.jpg" width="40%">

ä¸‰æ­¥ä¸Šæ‰‹ LLaMA2ï¼Œä¸€èµ·ç©ï¼ç›¸å…³åšå®¢æ•™ç¨‹å·²æ›´æ–°ï¼Œ**åŒæ ·æ¬¢è¿â€œä¸€é”®ä¸‰è¿â€** ğŸŒŸğŸŒŸğŸŒŸã€‚

> ä½¿ç”¨ Docker å¿«é€Ÿä¸Šæ‰‹ï¼Œæœ¬åœ°éƒ¨ç½² 7B æˆ– 13B å®˜æ–¹æ¨¡å‹ï¼Œæˆ–è€… 7B ä¸­æ–‡æ¨¡å‹ã€‚

- Meta Llama2 æ¨¡å‹ï¼Œ ä½¿ç”¨ 4090 éªŒè¯ï¼Œéœ€è¦ 8~14GB æ˜¾å­˜
- ä¸­æ–‡ Llama2 æ¨¡å‹ï¼Œä½¿ç”¨ 4090 éªŒè¯ï¼Œéœ€è¦ 8~14GB æ˜¾å­˜
- é‡åŒ–åçš„ä¸­æ–‡ Llama2 æ¨¡å‹ï¼Œä½¿ç”¨ 4090 éªŒè¯ï¼Œéœ€è¦ 5GB æ˜¾å­˜
- ä½¿ç”¨ GGML (llama.cpp) æ¨¡å‹ï¼Œåªéœ€è¦ CPU å°±èƒ½å¤Ÿè¿è¡Œæ¨¡å‹

ä½ å¯ä»¥å‚è€ƒé¡¹ç›®ä»£ç ï¼Œä¸¾ä¸€åä¸‰ï¼ŒæŠŠæ¨¡å‹è·‘èµ·æ¥ï¼Œæ¥å…¥åˆ°ä½ æƒ³ç©çš„åœ°æ–¹ï¼ŒåŒ…æ‹¬å¹¶ä¸å±€é™äºæ”¯æŒ LLaMA 1ä»£çš„å„ç§å¼€æºè½¯ä»¶ä¸­ã€‚

## é¢„è§ˆå›¾

![](.github/preview.png)

![](.github/llama2-cn-4bit.jpg)

![](.github/clip.gif)

## åšå®¢æ•™ç¨‹

- [ä½¿ç”¨ Docker å¿«é€Ÿä¸Šæ‰‹å®˜æ–¹ç‰ˆ LLaMA2 å¼€æºå¤§æ¨¡å‹](https://soulteary.com/2023/07/21/use-docker-to-quickly-get-started-with-the-official-version-of-llama2-open-source-large-model.html)
- [ä½¿ç”¨ Docker å¿«é€Ÿä¸Šæ‰‹ä¸­æ–‡ç‰ˆ LLaMA2 å¼€æºå¤§æ¨¡å‹](https://soulteary.com/2023/07/21/use-docker-to-quickly-get-started-with-the-chinese-version-of-llama2-open-source-large-model.html)
- [ä½¿ç”¨ Transformers é‡åŒ– Meta AI LLaMA2 ä¸­æ–‡ç‰ˆå¤§æ¨¡å‹](https://soulteary.com/2023/07/22/quantizing-meta-ai-llama2-chinese-version-large-models-using-transformers.html)
- [æ„å»ºèƒ½å¤Ÿä½¿ç”¨ CPU è¿è¡Œçš„ MetaAI LLaMA2 ä¸­æ–‡å¤§æ¨¡å‹](https://soulteary.com/2023/07/23/build-llama2-chinese-large-model-that-can-run-on-cpu.html)


## ä½¿ç”¨æ–¹æ³•

1. ä¸€æ¡å‘½ä»¤ï¼Œä»é¡¹ç›®ä¸­æ„å»ºå®˜æ–¹ç‰ˆï¼ˆ7Bæˆ–13Bï¼‰æ¨¡å‹é•œåƒï¼Œæˆ–ä¸­æ–‡ç‰ˆé•œåƒï¼ˆ7Bæˆ–INT4é‡åŒ–ç‰ˆï¼‰ï¼š

```bash
# 7B
bash scripts/make-7b.sh

# æˆ– 13B
bash scripts/make-13b.sh

# æˆ– 7B Chinese
bash scripts/make-7b-cn.sh

# æˆ– 7B Chinese 4bit
bash scripts/make-7b-cn-4bit.sh
```

2. é€‰æ‹©é€‚åˆä½ çš„å‘½ä»¤ï¼Œä» HuggingFace ä¸‹è½½ LLaMA2 æˆ–ä¸­æ–‡æ¨¡å‹ï¼š

```bash
# MetaAI LLaMA2 Models (10~14GB vRAM)
git clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
git clone https://huggingface.co/meta-llama/Llama-2-13b-chat-hf

mkdir meta-llama
mv Llama-2-7b-chat-hf meta-llama/
mv Llama-2-13b-chat-hf meta-llama/

# æˆ– Chinese LLaMA2 (10~14GB vRAM)
git clone https://huggingface.co/LinkSoul/Chinese-Llama-2-7b

mkdir LinkSoul
mv Chinese-Llama-2-7b LinkSoul/

# æˆ– Chinese LLaMA2 4BIT (5GB vRAM)
git clone https://huggingface.co/soulteary/Chinese-Llama-2-7b-4bit

mkdir soulteary
mv Chinese-Llama-2-7b-4bit soulteary/
```

å°†ä¸‹è½½å¥½çš„æ¨¡å‹ï¼Œä¿æŒåœ¨ä¸€ä¸ªæ­£ç¡®çš„ç›®å½•ç»“æ„ä¸­ã€‚

```bash
tree -L 2 meta-llama
soulteary
â””â”€â”€ ...
LinkSoul
â””â”€â”€ ...
meta-llama
â”œâ”€â”€ Llama-2-13b-chat-hf
â”‚Â Â  â”œâ”€â”€ added_tokens.json
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ generation_config.json
â”‚Â Â  â”œâ”€â”€ LICENSE.txt
â”‚Â Â  â”œâ”€â”€ model-00001-of-00003.safetensors
â”‚Â Â  â”œâ”€â”€ model-00002-of-00003.safetensors
â”‚Â Â  â”œâ”€â”€ model-00003-of-00003.safetensors
â”‚Â Â  â”œâ”€â”€ model.safetensors.index.json
â”‚Â Â  â”œâ”€â”€ pytorch_model-00001-of-00003.bin
â”‚Â Â  â”œâ”€â”€ pytorch_model-00002-of-00003.bin
â”‚Â Â  â”œâ”€â”€ pytorch_model-00003-of-00003.bin
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin.index.json
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ Responsible-Use-Guide.pdf
â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
â”‚Â Â  â”œâ”€â”€ tokenizer.model
â”‚Â Â  â””â”€â”€ USE_POLICY.md
â””â”€â”€ Llama-2-7b-chat-hf
    â”œâ”€â”€ added_tokens.json
    â”œâ”€â”€ config.json
    â”œâ”€â”€ generation_config.json
    â”œâ”€â”€ LICENSE.txt
    â”œâ”€â”€ model-00001-of-00002.safetensors
    â”œâ”€â”€ model-00002-of-00002.safetensors
    â”œâ”€â”€ model.safetensors.index.json
    â”œâ”€â”€ models--meta-llama--Llama-2-7b-chat-hf
    â”œâ”€â”€ pytorch_model-00001-of-00003.bin
    â”œâ”€â”€ pytorch_model-00002-of-00003.bin
    â”œâ”€â”€ pytorch_model-00003-of-00003.bin
    â”œâ”€â”€ pytorch_model.bin.index.json
    â”œâ”€â”€ README.md
    â”œâ”€â”€ special_tokens_map.json
    â”œâ”€â”€ tokenizer_config.json
    â”œâ”€â”€ tokenizer.json
    â”œâ”€â”€ tokenizer.model
    â””â”€â”€ USE_POLICY.md
```

3. é€‰æ‹©ä½¿ç”¨ä¸‹é¢çš„é€‚åˆä½ çš„å‘½ä»¤ï¼Œä¸€é”®è¿è¡Œ LLaMA2 æ¨¡å‹åº”ç”¨ï¼š

```bash
# 7B
bash scripts/run-7b.sh
# æˆ– 13B
bash scripts/run-13b.sh
# æˆ– Chinese 7B
bash scripts/run-7b-cn.sh
# æˆ– Chinese 7B 4BIT
bash scripts/run-7b-cn-4bit.sh
```

æ¨¡å‹è¿è¡Œä¹‹åï¼Œåœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost7860` æˆ–è€… `http://ä½ çš„IPåœ°å€:7860` å°±å¯ä»¥å¼€å§‹ç©äº†ã€‚

## ç›¸å…³é¡¹ç›®

- MetaAI LLaMA2: https://ai.meta.com/llama/ â¤ï¸
- Meta LLaMA2 7B Chat: https://huggingface.co/meta-llama/Llama-2-7b-chat
- Meta LLaMA2 13B Chat: https://huggingface.co/meta-llama/Llama-2-13b-chat
- Chinese LLaMA2 7B: https://huggingface.co/LinkSoul/Chinese-Llama-2-7b â¤ï¸
- Chinese LLaMA2 7B GGML q4: https://huggingface.co/soulteary/Chinese-Llama-2-7b-ggml-q4
- LLaMA2 GGML Converter: https://hub.docker.com/r/soulteary/llama2
